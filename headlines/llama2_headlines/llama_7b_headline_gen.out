Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]Loading checkpoint shards:   7%|▋         | 1/14 [00:04<00:54,  4.20s/it]Loading checkpoint shards:  14%|█▍        | 2/14 [00:09<00:56,  4.67s/it]Loading checkpoint shards:  21%|██▏       | 3/14 [00:13<00:50,  4.63s/it]Loading checkpoint shards:  29%|██▊       | 4/14 [00:18<00:47,  4.72s/it]Loading checkpoint shards:  36%|███▌      | 5/14 [00:22<00:40,  4.53s/it]Loading checkpoint shards:  43%|████▎     | 6/14 [00:26<00:33,  4.23s/it]Loading checkpoint shards:  50%|█████     | 7/14 [00:30<00:29,  4.20s/it]Loading checkpoint shards:  57%|█████▋    | 8/14 [00:35<00:26,  4.38s/it]Loading checkpoint shards:  64%|██████▍   | 9/14 [00:40<00:22,  4.54s/it]Loading checkpoint shards:  71%|███████▏  | 10/14 [00:44<00:17,  4.48s/it]Loading checkpoint shards:  79%|███████▊  | 11/14 [00:49<00:13,  4.62s/it]Loading checkpoint shards:  86%|████████▌ | 12/14 [00:54<00:09,  4.69s/it]Loading checkpoint shards:  93%|█████████▎| 13/14 [00:58<00:04,  4.57s/it]Loading checkpoint shards: 100%|██████████| 14/14 [01:02<00:00,  4.36s/it]Loading checkpoint shards: 100%|██████████| 14/14 [01:02<00:00,  4.47s/it]
Token indices sequence length is longer than the specified maximum sequence length for this model (1788 > 1024). Running this sequence through the model will result in indexing errors
This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.
>> Results dumped to llama2_headlines folder!
