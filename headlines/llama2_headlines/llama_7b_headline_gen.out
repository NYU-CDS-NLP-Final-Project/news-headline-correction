Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]Loading checkpoint shards:   7%|▋         | 1/14 [00:01<00:21,  1.62s/it]Loading checkpoint shards:  14%|█▍        | 2/14 [00:03<00:19,  1.60s/it]Loading checkpoint shards:  21%|██▏       | 3/14 [00:04<00:17,  1.60s/it]Loading checkpoint shards:  29%|██▊       | 4/14 [00:06<00:16,  1.62s/it]Loading checkpoint shards:  36%|███▌      | 5/14 [00:08<00:14,  1.60s/it]Loading checkpoint shards:  43%|████▎     | 6/14 [00:09<00:12,  1.62s/it]Loading checkpoint shards:  50%|█████     | 7/14 [00:11<00:11,  1.63s/it]Loading checkpoint shards:  57%|█████▋    | 8/14 [00:12<00:09,  1.63s/it]Loading checkpoint shards:  64%|██████▍   | 9/14 [00:14<00:08,  1.65s/it]Loading checkpoint shards:  71%|███████▏  | 10/14 [00:16<00:06,  1.63s/it]Loading checkpoint shards:  79%|███████▊  | 11/14 [00:17<00:04,  1.63s/it]Loading checkpoint shards:  86%|████████▌ | 12/14 [00:19<00:03,  1.63s/it]Loading checkpoint shards:  93%|█████████▎| 13/14 [00:21<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 14/14 [00:22<00:00,  1.57s/it]Loading checkpoint shards: 100%|██████████| 14/14 [00:22<00:00,  1.61s/it]
Token indices sequence length is longer than the specified maximum sequence length for this model (1788 > 1024). Running this sequence through the model will result in indexing errors
This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.
>> Results dumped to llama2 folder!
